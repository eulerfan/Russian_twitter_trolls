---
title: "Russian Twitter Trolls"
output: github_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Background

After extensive government investigations, it was determined that Russia and Iran tried to influence the U.S. 2016 presidential elections through social media platforms. To work with the government in a constructive manner and keep its platform in a positive public light, Twitter made more than ten million foreign troll’s tweets available for research. A team at NBC News reconstructed a dataset of 200,000 Russian troll tweets and made it available on Kaggle. Their research showed these troll accounts were extremely active during key moments around the election. 
This project is attempting to determine which tweets are Russian troll tweets and which are not Russian troll tweets. It will be based on three predictive models, Naïve Bayes, Logistic Regression, and Random Forest.  Then we will determine which predictive model is most effective when filtering the Russian troll tweets. This could be one of many possible filtering algorithms to help Twitter in identifying fake accounts of Russian or even Iranian trolls. There are two major reasons for Twitter to identify fake accounts: 

1. Rebuilding public trust in their platform, and
2. influence the government’s legislation regulating social media

For more background, read the NBC news article publicizing the release: "Twitter deleted 200,000 Russian troll tweets."[NBC Russian Tweets]( https://www.nbcnews.com/tech/social-media/now-available-more-200-000-deleted-russian-troll-tweets-n844731)

## Content
This dataset contains two CSV files. tweets.csv includes details on individual tweets from known Russain troll accounts,and noemoticon.csv file that is pre-russain trolls(2009).

To recreate a link to an individual tweet found in the dataset, replace user_key in https://twitter.com/user_key/status/tweet_id with the screen-name from the user_key field and tweet_id with the number in the tweet_id field.

Following the links will lead to a suspended page on Twitter. But some copies of the tweets as they originally appeared, including images, can be found by entering the links on web caches like archive.org and archive.is.

Acknowledgements
If you publish using the data, please credit NBC News and include a link to this page. Send questions to ben.popken@nbcuni.com. 

## This Project

In this markdown document, the russian troll data set and a sentiment data set that was pre 2014 was used for analysis. The Russian troll accounts were from September 2014 until September 2017. So I found tweets that were produced from April to May 2009. Therefore, camparisons of troll and non-troll data could be made.    

```{r, echo=FALSE, warning= FALSE}
library(ROCR)
library(caTools)# ROC, AUC
library(e1071) #  Naive Bayes 
library(SnowballC) # stemming
library(gmodels)
library(tidytext)
library(dplyr)
library(ggplot2)
library(wordcloud)
library(tidyr)
library(tm) # text data
library(effects) # regession models
library(glm2)


#nt<-read.csv("C:/Users/John/Documents/R/russian_trolls/training.1600000.processed.noemoticon.csv")
#rt <-read.csv("file:///C:/Users/John/Documents/R/russian_trolls/tweets.csv/tweets.csv")

#set.seed(33)
#rt <- data.frame(sample_n(rt,6000,replace=FALSE))
#nt<-data.frame(sample_n(nt,6000,replace = FALSE))

#save(rt,file = "savedrt.RData")
#save(nt,file= "savednt.RData")


```

Because the CSV files were so large, two Rdata files were created. This helped with shorter run time and smaller upload to Github. The text column was extracted from both data sets, then labels were added. The Russian troll text labeled r, the text with no Russian trolls was labeled nr. The column heading is r_nr. 
```{r}

load("savednt.RData")

load("savedrt.RData")

#renaming Features
created_str <- as.Date(rt$created_str)
text <- as.character(rt$text)
colnames(nt)[6] <- as.character(c("text"))
colnames(nt)[3] <- "created_str"

#extracting columns
rtext <- select(rt,"text")
ntext <- select(nt,"text")


#adding a column Russian tweets
rtext$r_nr <- "r"

#adding column to non_russian tweets
ntext$r_nr <- "nr"


str(rt)
str(nt)
```

With the new data set, a corpus is created and gets cleaned by removing numbers, stopwords, white space, and punctuations.  We also need to make all words lower case and stem the words.   

```{r Text Tokenizing}

#combining data sets, sample function randomizes order of rows

tot_tweets <- rbind(ntext,rtext,stringsAsFactors = FALSE) 
tot_tweets <- tot_tweets[sample(nrow(tot_tweets)),]

#change character to factor
tot_tweets$r_nr <- factor(tot_tweets$r_nr, levels = c("r","nr"),ordered = FALSE)

#creating a corpus for the twitter text
text_corpus <- VCorpus(VectorSource(tot_tweets$text))
print(text_corpus)

#cleaning tweets 
text_corpus_clean <- tm_map(text_corpus,content_transformer(tolower))

text_corpus_clean <- tm_map(text_corpus_clean,removeNumbers)

text_corpus_clean <- tm_map(text_corpus_clean,removeWords,stopwords())

text_corpus_clean <- tm_map(text_corpus_clean,removePunctuation)

text_corpus_clean <- tm_map(text_corpus_clean,stemDocument)
                                                                                                         
text_corpus_clean <- tm_map(text_corpus_clean,stripWhitespace)


#Tokenize the data
text_dtm <- DocumentTermMatrix(text_corpus_clean,control =
                                list(wordLengths = c(1,Inf))) 


```

We separate the data into a training set and a test set. Then create the labels for the two sets. 
```{r Separate Training and Testing}
text_dtm_train <- text_dtm[1:10000,]
text_dtm_test <- text_dtm[10001:12000,]


text_train_labels <- tot_tweets[1:10000,]$r_nr
text_test_labels <- tot_tweets[10001:12000,]$r_nr


```


We can now create three word clouds. The first wordcloud shows all of the data together. The second cloud show just the russian troll texts. The third is no Russian trolls at all.
```{r Create Word Clouds}
#Overall word graph
wordcloud(text_corpus_clean, min.freq = 100,scale = c(2,.5),random.order = FALSE)

#Russian troll graph
rus <- subset(tot_tweets,r_nr == "r")
norus <- subset(tot_tweets,r_nr == "nr")

wordcloud(rus$text, max.words = 40,scale = c(3,.5))

#no russian trolls graph
wordcloud(norus$text, max.words = 40, scale = c(3,.5))
```

Now we decide the frequency of the words that will be changed into a matrix. A  document-term matrix is created so we can analyze the text.

```{r echo=TRUE, message=FALSE, warning=FALSE}

 
#number of frequent terms. frenquency filter of words used less than 20 times
tweet_freq_words <- findFreqTerms(text_dtm_train, 20)
 
 str(tweet_freq_words)
 
#create DTM
 
tweet_dtm_freq_train <- text_dtm_train[ , tweet_freq_words]
tweet_dtm_freq_test <- text_dtm_test[ , tweet_freq_words]

#Function for Change to categorical classifier, then apply to the columns
 convert_counts <- function(x) {
 x <- ifelse(x > 0, "Yes", "No")
 }

tweet_train <- apply(tweet_dtm_freq_train, MARGIN = 2,
 convert_counts)
tweet_test <- apply(tweet_dtm_freq_test, MARGIN = 2,
 convert_counts)
  
tweet_classifier <- naiveBayes(tweet_train,text_train_labels)

tweet_test_pred <- predict(tweet_classifier, tweet_test)

 CrossTable(tweet_test_pred, text_test_labels,
 prop.chisq = FALSE, prop.t = FALSE,
 dnn = c('predicted', 'actual'))

 
```

This the NaIve Bayes model, and from the cross table, we can see that we have an accuracy of 93%. 


```{r ROC for NB}

#create ROC for Naive Bayes
troc <- predict(tweet_classifier,tweet_test, type = "raw")
predt <- prediction(troc[,"r"],text_test_labels)



#calculate the area
auc <- performance(predt,"auc")


perf_r <- performance(predt, measure = 'tpr',x.measure = 'fpr')
plot(perf_r,colorize = T,main = "Naive Bayes")
    
print(auc)


```

```{r Logistic Regression, echo=TRUE}

library(caret)
library(safeBinaryRegression)
library(glmnet)
library(Matrix)
library(pROC)
library(ROSE)

 #logistic Regression

sparse_dtm <- removeSparseTerms(text_dtm, 0.995) #terms appear in more than .05% of tweets or 230 terms 

#new Data frame
tweetsSparse <- as.data.frame(as.matrix(sparse_dtm))
colnames(tweetsSparse) <- make.names(colnames(tweetsSparse))
tweetsSparse$r_nr <- tot_tweets$r_nr

#split the data set

trainSparse <- tweetsSparse[1:9000,]
testS <- tweetsSparse[9001:12000,]

#logistic regression model
tweet.logit <- glm2(r_nr~.,trainSparse, family = "binomial")

tweet.logit.test <- predict(tweet.logit,type = "response", newdata = testS,na.action = na.exclude)

cmatrix_logregr <- table(testS$r_nr, tweet.logit.test > 0.5)

cmatrix_logregr


#create ROC and auc for Logistic Regression

logRe <- roc.curve(testS$r_nr,tweet.logit.test,main = "Logistic Regression")
logRe


```
To create a Logistic Regression model, we removed the sparse terms of greater than 99.995%.  We split the new data and run the model to get accuracy 92%. 

```{r Decision Tree}
#Descision Tree
library(rpart)
library(rpart.plot)
library(e1071)
library(irr)

tweetcart <- rpart(r_nr~.,data = trainSparse, method = "class")
prp(tweetcart)

predtcart <- predict(tweetcart,newdata = testS,type = "class",na.action = na.omit)
cartable <- table(testS$r_nr,predtcart)

print(cartable)

#cross-validation

tr.control <- trainControl(method = "cv",number = 5,classProbs = TRUE,
                          summaryFunction = twoClassSummary) 
cp.grid <- expand.grid(.cp = (0:5)*0.001)
tr <- train(r_nr~.,data = tweetsSparse, method = "rpart", trControl = tr.control,tuneGrid = cp.grid, na.action = na.omit,
          metric = "ROC" )
tr

```
The Decision Tree model on a binary classifier is just one branch that would eventually change directions. It uses the same sparse data as Logisitc Regression, which will also be used for Random Forest.Then I used cross-validation to perform an even better check of the data on the model. In the end, the cp of 0.00  was the best fit with an accuracy of 91%.

```{r Random Forest}

#Random Forest
library(randomForest)
russianForest <- randomForest(r_nr~.,data = trainSparse,nodesize = 25,ntrees = 200,na.action = na.roughfix)
predictForest <- predict(russianForest, newdata = testS)

table(testS$r_nr,predictForest)

#random forest ROC
rfroc <- prediction(as.numeric(predictForest),as.numeric(testS$r_nr))
perf_3 <- performance(rfroc,measure = 'tpr',x.measure = 'fpr')
plot(perf_3, colorize = T, main = "Random Forest")

auc3 <- performance(rfroc,"auc")
print(auc3)

```
The last model is Random Forest, which is several descion trees, had an accuracy of 92%.

## Model Comparison

When we compare all of these models for accuracy and auc, we find that all the models used perfoemd well. The Naive Bayes model was slightly more accurate; It's ROC/AUC shows that this model would perform better than the other three models in classifying tweets. The decision tree model showd that containing the nouns "Trump", "Clinton" where the most indicative in deciding if a the tweet was written by a troll or not. But this doesn't add much to our insight. As a means of testness, we applied cross validation to one of the models and it didn't show drastic difference in performance.

## Future imporvements

Models based on two-word and three-word combinations could improve accuracy of troll detection.  Geospatial addresses could improve these models. Also adding network analysis to see the pattern that these tweets are initiated and spread might give more insights

As time goes on the trolls will get better and better at masking their efforts to meddle in elections across the world.  This will be an ongoing effort for Twiter and other social media outlets to create more models with more accuracy.  